{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yidinghou/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import player\n",
    "import game\n",
    "import neural_network\n",
    "import mcts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import board as b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree, edge_statistics = mcts.MCTS.get_tree_and_edges()\n",
    "board = b.Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16167"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_statistics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "keys = list(edge_statistics.keys())\n",
    "for key in keys:\n",
    "    state = edge_statistics[key]\n",
    "    initial_state, final_state = key.split(\"2\")\n",
    "    initial_arr = board.str2arr(initial_state)\n",
    "    final_arr = board.str2arr(final_state)\n",
    "    \n",
    "    p_type = (final_arr - initial_arr).sum()\n",
    "    \n",
    "    X.append(final_arr)\n",
    "    Y.append(state[\"Q\"]*p_type)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y_norm = (Y+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    0,     0,     0, ..., 16166, 16166, 16166]),\n",
       " array([0, 0, 0, ..., 1, 2, 2]),\n",
       " array([0, 1, 2, ..., 2, 0, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(X==np.array([[ 1, -1,  1],\n",
    "                      [-1,  0,  0],\n",
    "                      [ 0,  1, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(np.array([[ 1, -1,  1],\n",
    "                      [-1,  0,  0],\n",
    "                      [ 0,  1, -1]]), X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11285]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,x in enumerate(X) if np.array_equal(np.array([[ 0,  0,  0],\n",
    "                                                       [ 0,  1,  0],\n",
    "                                                       [ 0,  0, 0]]), x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[11285]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'equals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-95a01c85ca08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m X[0].equals(np.array([[ 1, -1,  1],\n\u001b[0m\u001b[1;32m      2\u001b[0m                       \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       [ 0,  1, -1]]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'equals'"
     ]
    }
   ],
   "source": [
    "X[0].equals(np.array([[ 1, -1,  1],\n",
    "                      [-1,  0,  0],\n",
    "                      [ 0,  1, -1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos = [i for i,y in enumerate(Y) if y ==1]\n",
    "Y_neg = [i for i,y in enumerate(Y) if y ==-1]\n",
    "Y_drw = [i for i,y in enumerate(Y) if y ==0]\n",
    "\n",
    "Y_clean = np.concatenate([Y[Y_pos], \n",
    "                          Y[Y_neg],\n",
    "                          Y[Y_drw],\n",
    "                         ])\n",
    "\n",
    "X_clean = np.concatenate([X[Y_pos], \n",
    "                          X[Y_neg],\n",
    "                          X[Y_drw],\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "nb_classes = 3\n",
    "targets = (Y_clean+1)\n",
    "one_hot_targets = np.eye(nb_classes)[targets.astype(int)]\n",
    "one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 3, 3, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 3, 1, 3)      12          input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 1, 3, 3)      12          input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 1, 1, 4)      40          input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling2D) (None, 1, 1, 3)      0           conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling2D) (None, 1, 1, 3)      0           conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling2D) (None, 1, 1, 4)      0           conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 1, 1, 10)     0           max_pooling2d_73[0][0]           \n",
      "                                                                 max_pooling2d_74[0][0]           \n",
      "                                                                 max_pooling2d_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 10)           0           concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 10)           110         flatten_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "V (Dense)                       (None, 3)            33          dense_21[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 207\n",
      "Trainable params: 207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, Flatten, Conv2D, MaxPooling1D, Dropout, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "Input_1= Input(shape=(3,3,1))\n",
    "\n",
    "x1 = Conv2D(filters = 3, kernel_size=(1,3), activation='relu', \n",
    "            kernel_regularizer=l2(0.0005),\n",
    "            kernel_initializer=initializers.RandomNormal(stddev=0.1, mean=0),\n",
    "            input_shape=(3,3,1))(Input_1)\n",
    "\n",
    "x2 = Conv2D(filters = 3, kernel_size=(3,1), activation='relu', \n",
    "            kernel_regularizer=l2(0.0005),\n",
    "            kernel_initializer=initializers.RandomNormal(stddev=0.1, mean=0),\n",
    "            input_shape=(3,3,1))(Input_1)\n",
    "\n",
    "x3 = Conv2D(filters = 4, kernel_size=(3,3), activation='relu', \n",
    "            kernel_regularizer=l2(0.0005),\n",
    "            kernel_initializer=initializers.RandomNormal(stddev=0.1, mean=0),\n",
    "            input_shape=(3,3,1))(Input_1)\n",
    "\n",
    "x1 = MaxPooling2D((3, 1))(x1)\n",
    "x2 = MaxPooling2D((1, 3))(x2)\n",
    "x3 = MaxPooling2D((1,1))(x3)\n",
    "\n",
    "\n",
    "x = Concatenate()([x1, x2, x3])\n",
    "# x = MaxPooling2D((3,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "value_head = Dense(10,  activation='relu')(x)\n",
    "# value_head = Dense(10,  activation='relu')(x)\n",
    "value_head = Dense(3,  activation='softmax', name = \"V\")(value_head)\n",
    "\n",
    "model = Model(inputs=Input_1, outputs=value_head)\n",
    "\n",
    "opt = SGD(lr=0.01, momentum=0.09)\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                             \n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos = [i for i,y in enumerate(Y_clean) if y ==1]\n",
    "Y_neg = [i for i,y in enumerate(Y_clean) if y ==-1]\n",
    "Y_drw = [i for i,y in enumerate(Y_clean) if y ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3, 3, 1)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([X_final], axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7737 - acc: 0.6550\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7676 - acc: 0.6640\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7643 - acc: 0.6603\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7631 - acc: 0.6630\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 64us/sample - loss: 0.7598 - acc: 0.6647\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7589 - acc: 0.6613\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.7567 - acc: 0.6600\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.7556 - acc: 0.6590\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 60us/sample - loss: 0.7542 - acc: 0.6657\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7534 - acc: 0.6670\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7739 - acc: 0.6550\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7700 - acc: 0.6553\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7675 - acc: 0.6607\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7652 - acc: 0.6613\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.7623 - acc: 0.6657\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7633 - acc: 0.6587\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7611 - acc: 0.6630\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7590 - acc: 0.6637\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.7590 - acc: 0.6623\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7579 - acc: 0.6587\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7840 - acc: 0.6417\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 60us/sample - loss: 0.7815 - acc: 0.6493\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.7795 - acc: 0.6430\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7775 - acc: 0.6527\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7764 - acc: 0.6510\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7745 - acc: 0.6540\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.7746 - acc: 0.6547\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7722 - acc: 0.6570\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7732 - acc: 0.6567\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7716 - acc: 0.6563\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7646 - acc: 0.6703\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7615 - acc: 0.6677\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7599 - acc: 0.6740\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7579 - acc: 0.6757\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.7561 - acc: 0.6697\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7564 - acc: 0.6693\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7555 - acc: 0.6730\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7536 - acc: 0.6747\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7531 - acc: 0.6760\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7526 - acc: 0.6703\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.7693 - acc: 0.6567\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.7638 - acc: 0.6610\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7623 - acc: 0.6637\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7616 - acc: 0.6693\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7591 - acc: 0.6637\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.7582 - acc: 0.6693\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7573 - acc: 0.6680\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7556 - acc: 0.6703\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7545 - acc: 0.6683\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7537 - acc: 0.6703\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7716 - acc: 0.6580\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7681 - acc: 0.6557\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7646 - acc: 0.6657\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7636 - acc: 0.6620\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7614 - acc: 0.6607\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7607 - acc: 0.6610\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7589 - acc: 0.6643\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7584 - acc: 0.6673\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7559 - acc: 0.6633\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7550 - acc: 0.6617\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.7539 - acc: 0.6713\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7491 - acc: 0.6740\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7443 - acc: 0.6770\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7434 - acc: 0.6730\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7420 - acc: 0.6763\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7411 - acc: 0.6773\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.7400 - acc: 0.6770\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7389 - acc: 0.6753\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7390 - acc: 0.6757\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7375 - acc: 0.6760\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.7582 - acc: 0.6633\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.7516 - acc: 0.6663\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.7489 - acc: 0.6693\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7470 - acc: 0.6680\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7448 - acc: 0.6620\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.7431 - acc: 0.6677\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 62us/sample - loss: 0.7420 - acc: 0.6620\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.7401 - acc: 0.6727\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7391 - acc: 0.6657\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7394 - acc: 0.6663\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7349 - acc: 0.6847\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7283 - acc: 0.6807\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.7261 - acc: 0.6847\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7234 - acc: 0.6807\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.7197 - acc: 0.6893\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7198 - acc: 0.6867\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.7190 - acc: 0.6917\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7186 - acc: 0.6880\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.7159 - acc: 0.6910\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7165 - acc: 0.6937\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7683 - acc: 0.6637\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7617 - acc: 0.6627\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7573 - acc: 0.6647\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7563 - acc: 0.6703\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7546 - acc: 0.6670\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7519 - acc: 0.6727\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7519 - acc: 0.6727\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7503 - acc: 0.6747\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.7502 - acc: 0.6720\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.7495 - acc: 0.6767\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.7471 - acc: 0.6737\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7406 - acc: 0.6720\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7372 - acc: 0.6733\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7349 - acc: 0.6763\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7308 - acc: 0.6773\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7291 - acc: 0.6747\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7277 - acc: 0.6800\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.7273 - acc: 0.6850\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.7257 - acc: 0.6827\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.7248 - acc: 0.6810\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7261 - acc: 0.6910\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.7200 - acc: 0.6920\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.7176 - acc: 0.6967\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.7147 - acc: 0.6943\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.7143 - acc: 0.6937\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7122 - acc: 0.6927\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7107 - acc: 0.6953\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.7093 - acc: 0.6967\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.7083 - acc: 0.6987\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7085 - acc: 0.6920\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7157 - acc: 0.6930\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7097 - acc: 0.6947\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.7084 - acc: 0.6993\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.7071 - acc: 0.6957\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.7056 - acc: 0.6963\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7042 - acc: 0.7007\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7025 - acc: 0.7050\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7040 - acc: 0.6983\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7021 - acc: 0.6980\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7001 - acc: 0.7040\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7371 - acc: 0.6680\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7324 - acc: 0.6687\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7307 - acc: 0.6627\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7283 - acc: 0.6667\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7267 - acc: 0.6687\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7262 - acc: 0.6687\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7243 - acc: 0.6730\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.7232 - acc: 0.6710\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7235 - acc: 0.6687\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7227 - acc: 0.6753\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7222 - acc: 0.6790\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7155 - acc: 0.6867\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7119 - acc: 0.6877\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7092 - acc: 0.6953\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.7064 - acc: 0.6877\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.7059 - acc: 0.6923\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.7056 - acc: 0.6927\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.7039 - acc: 0.6930\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7018 - acc: 0.6977\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7011 - acc: 0.6940\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.7205 - acc: 0.6887\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.7153 - acc: 0.6943\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.7125 - acc: 0.6930\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7106 - acc: 0.6983\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.7091 - acc: 0.6960\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.7068 - acc: 0.6963\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 70us/sample - loss: 0.7066 - acc: 0.6953\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 75us/sample - loss: 0.7062 - acc: 0.6943\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.7038 - acc: 0.6970\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.7024 - acc: 0.6977\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.6788 - acc: 0.7097\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.6739 - acc: 0.7137\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.6711 - acc: 0.7147\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.6706 - acc: 0.7150\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.6681 - acc: 0.7177\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.6669 - acc: 0.7167\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.6656 - acc: 0.7200\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.6647 - acc: 0.7210\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.6632 - acc: 0.7223\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.6624 - acc: 0.7243\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.7217 - acc: 0.6753\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.7153 - acc: 0.6823\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.7124 - acc: 0.6800\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.7102 - acc: 0.6770\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.7091 - acc: 0.6827\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.7073 - acc: 0.6790\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.7061 - acc: 0.6863\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.7039 - acc: 0.6860\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.7054 - acc: 0.6817\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.7011 - acc: 0.6850\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.6976 - acc: 0.7003\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.6912 - acc: 0.7037\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.6881 - acc: 0.7020\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.6857 - acc: 0.7107\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.6824 - acc: 0.7087\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.6825 - acc: 0.7100\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.6793 - acc: 0.7120\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.6792 - acc: 0.7147\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.6772 - acc: 0.7163\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.6773 - acc: 0.7170\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.6965 - acc: 0.7000\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.6903 - acc: 0.6963\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.6890 - acc: 0.7033\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.6864 - acc: 0.7010\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 58us/sample - loss: 0.6843 - acc: 0.7017\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.6838 - acc: 0.7013\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.6816 - acc: 0.7033\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 58us/sample - loss: 0.6802 - acc: 0.6980\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.6798 - acc: 0.7073\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 58us/sample - loss: 0.6786 - acc: 0.6983\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.7039 - acc: 0.7013\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.6995 - acc: 0.7017\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.6974 - acc: 0.6993\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.6964 - acc: 0.6950\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.6954 - acc: 0.6943\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.6944 - acc: 0.7013\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.6932 - acc: 0.7027\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.6931 - acc: 0.6967\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.6910 - acc: 0.6943\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.6897 - acc: 0.6990\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.6941 - acc: 0.6980\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 61us/sample - loss: 0.6914 - acc: 0.6907\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 58us/sample - loss: 0.6878 - acc: 0.6913\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.6887 - acc: 0.6990\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.6864 - acc: 0.6970\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 58us/sample - loss: 0.6845 - acc: 0.6920\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.6852 - acc: 0.6927\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.6834 - acc: 0.7003\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.6834 - acc: 0.6897\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 65us/sample - loss: 0.6829 - acc: 0.6957\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 60us/sample - loss: 0.6862 - acc: 0.7033\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.6833 - acc: 0.7057\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.6834 - acc: 0.7090\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.6802 - acc: 0.7133\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 61us/sample - loss: 0.6801 - acc: 0.7130\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 61us/sample - loss: 0.6785 - acc: 0.7150\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 65us/sample - loss: 0.6776 - acc: 0.7130\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.6763 - acc: 0.7157\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.6770 - acc: 0.7123\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.6742 - acc: 0.7160\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.6966 - acc: 0.7037\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.6943 - acc: 0.7047\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.6906 - acc: 0.7023\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.6910 - acc: 0.7023\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.6909 - acc: 0.7003\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.6884 - acc: 0.7057\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.6881 - acc: 0.7027\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.6883 - acc: 0.7077\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.6867 - acc: 0.7070\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.6862 - acc: 0.7023\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.6908 - acc: 0.7023\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.6866 - acc: 0.7043\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.6853 - acc: 0.7100\n",
      "Epoch 4/10\n",
      "  32/3000 [..............................] - ETA: 0s - loss: 0.4783 - acc: 0.8438"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    pos_sample = np.random.choice(Y_pos, 1000)\n",
    "    neg_sample = np.random.choice(Y_neg, 1000)\n",
    "    drw_sample = np.random.choice(Y_drw, 1000)\n",
    "\n",
    "\n",
    "    X_final = np.concatenate((X_clean[pos_sample], \n",
    "                              X_clean[neg_sample],\n",
    "                              X_clean[drw_sample]))   \n",
    "    \n",
    "    Y_final = np.concatenate((one_hot_targets[pos_sample], \n",
    "                              one_hot_targets[neg_sample],\n",
    "                              one_hot_targets[drw_sample]\n",
    "                             ))\n",
    "    \n",
    "    X_inp = np.stack([X_final], axis=-1)\n",
    "    model.fit(X_inp, Y_final, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAAyCAYAAABiUAEEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABv0lEQVR4nO3asWpTURzH8f+tIVARA21aB2my+RLq4NY3cPAxRB/E53HN4KAP4JYKDhoUBDMo7XFwkw5JOPUX9PMZL5f/OdwD34TLHVprBcDfd5DeAMD/SoABQgQYIESAAUIEGCBktM3Nx3cO29nR5Kb2spH13XvR9auqrsIfjnz6+KG+ff0y9Jo3nU7bfDbrNW4nV9HVf0v/G1leXNRqtep2rpNbo3Y6Gvcat5ODUfqp7of36++r1trJn9e3CvDZ0aReP3/Wb1c7eHv+Mrp+VdX6ZzYXL56ed503n81qsVh0nbmtH3tQ4PGQ3cTDR4+7zjsdjevV/QddZ27r9vQwun5VVbvMf2r75N2b5XXX/TwBhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAiAADhAyttc1vHobPVbW8ue2woXlr7aTXMOe6N5zrv+vas90qwAD04xUEQIgAA4QIMECIAAOECDBAiAADhAgwQIgAA4QIMEDIL+a0Rtw1lWyOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.76568466, -0.71651477, -0.7220862 ]], dtype=float32)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = model.layers[1].get_weights()[0]\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "n_filters, ix = 3, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(1):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='RdBu', vmin = -1, vmax = 1)\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "f[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARMAAABRCAYAAADxTkJrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACN0lEQVR4nO3cvWoUUQCG4bOaH8gmm5DFmJAEEbwCBQsLGztB8Q5SeAP21vY2lhZ7A4Ja21l5AxbaiIpJJGE1iSzqcmwsXfDghwvhedoZPg4MvMw006m1FoB/dWbaBwBOBzEBIsQEiBATIEJMgIiZlptXVvt1fXM7eoDdj+/L8PCgEx2lyczCcp3rrUU3v3/dLz+/ffFcp2hpZbX2N7aimwefPpSj4eEfn2tTTNY3t8vjZy8yp/rt7u0b0T3azfXWyqWdh9HNt4N70T3a9Te2yv3B8+jmg51bE6/5zAEixASIEBMgQkyACDEBIsQEiBATIEJMgAgxASLEBIgQEyBCTIAIMQEixASIEBMgQkyACDEBIsQEiBATIKLpH7Dd8Um5OnwVPUB3fBLdo93y0ny5ef1idHPwZD66R7uV471y52X2376PjvcmXvNmAkSICRAhJkCEmAARYgJEiAkQISZAhJgAEWICRIgJECEmQISYABFiAkSICRAhJkCEmAARYgJEiAkQISZAhJgAEWICRDT9nX40u1hen78WPcBodjG6R7vRj3F5s3sU32S6zi50S+/ylfDm04nXvJkAEWICRIgJECEmQISYABFiAkSICRAhJkCEmAARYgJEiAkQISZAhJgAEWICRIgJECEmQISYABFiAkSICRAhJkBEp9b69zd3Op9LKe/CZ7hQaz0X3qSB53o6/e/n2hQTgEl85gARYgJEiAkQISZAhJgAEWICRIgJECEmQISYABG/ACTdUol9TkloAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.3746033 ],\n",
       "       [-0.46866703],\n",
       "       [-0.30118838]], dtype=float32)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = model.layers[2].get_weights()[0]\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "n_filters, ix = 3, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(1):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='RdBu', vmin = -1, vmax = 1)\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "f[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAB5CAYAAABsi3e7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAD+UlEQVR4nO3dv2tdZRzH8efk5odp2iZNWtOkoVkc3NRRBIUiIiji5OSiDs4iKKiIW/8Au6iTky5CwaG4CdJFJ8HVSqzU0NQ2aZq016Y5/gMN9j6HkHwur9ea+833gcN933uWc5u2bQtAipGDPgDAIEQLiCJaQBTRAqKIFhBldJAXn5w+1i7Pz1Uv2xibrp4tpZSxXlM9+/dfV8v6zX/q/8EQOzEx3i5OTVbP9ybGOu0fnRyvnv1zbb3c2Nx2XR9iama2nT19pnr+5JFu13Wz/6B69vq1q+X2rZsPva4DRWt5fq5cvvBJ9UEuLb5SPVtKKQtHJ6pn33rtXKfdw2xxarJ889Jz1fMzT5zutH/2yeXq2ec//aLT7mE2e/pMee+ri9Xz7zyz0Gn/jysb1bPvv/Hynn9zewhEES0gimgBUUQLiCJaQBTRAqKIFhBFtIAoogVEES0gimgBUUQLiCJaQBTRAqIM9Gia3X6/3P3j9+pll26tVs+WUsrTZ09Uz27f3+20e5iNjPXKkcePV89PfvB5p/3NDxc6DPvc3Uv//m65cv1O9fzU+kqn/b+t9qpn7+7s/SwuVxyIIlpAFNECoogWEEW0gCiiBUQRLSCKaAFRRAuIIlpAFNECoogWEEW0gCiiBUQRLSDKQM/Tand3y87Wveplr7+wWD1bSikXf71WPbv9706n3cNsfPp4WXr1xer5jS8/6rR/9N3Pqmeb89922j3MmqaU3khTPd//6btO+y+vn6uevXNv7/erb1pAFNECoogWEEW0gCiiBUQRLSCKaAFRRAuIIlpAFNECoogWEEW0gCiiBUQRLSCKaAFRmrZtH/3FTbNWSlnZv+Psq+W2bU8d9CEOI9d1OA3rdR0oWgAHze0hEEW0gCiiBUQRLSCKaAFRRAuIIlpAlIF+rPXYzGw7t7C0X2f5X3PtVvXsyupaubF+u/6XK4FDYaBozS0slY+//r56Wa/p1ow37/9cPfvs2x922g0cDm4PgSiiBUQRLSCKaAFRRAuIIlpAFNECoogWEEW0gCiiBUQRLSCKaAFRRAuIMtBTHh60bdns71Qvmz86UT1bSint5naH4d1Ou4HDwTctIIpoAVFEC4giWkAU0QKiiBYQRbSAKKIFRBEtIIpoAVFEC4giWkAU0QKiiBYQRbSAKAM9T2urv1N+uXKzetlTZ2eqZ0sppXnsSIdhfYZh4J0MRBEtIIpoAVFEC4giWkAU0QKiiBYQRbSAKKIFRBEtIIpoAVFEC4giWkAU0QKiiBYQpWnb9tFf3DRrpZSV/TvOvlpu2/bUQR8C6GagaAEcNLeHQBTRAqKIFhBFtIAoogVEES0gimgBUUQLiCJaQJT/AHnQnI5lCGNBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.3682436 ,  0.3617187 , -0.28066802],\n",
       "       [ 0.40970123,  0.3379141 , -0.30520642],\n",
       "       [ 0.5409059 ,  0.4242979 , -0.27356228]], dtype=float32)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = model.layers[3].get_weights()[0]\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "n_filters, ix = 4, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(1):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='RdBu', vmin = -1, vmax = 1)\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "f[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "0.5783088235294118\n",
      "{'N': 21760, 'W': 15576, 'D': 3192, 'L': 2992, 'Q': 0.5783088235294118, 'P': 0}\n"
     ]
    }
   ],
   "source": [
    "key = keys[11285]\n",
    "state = edge_statistics[key]\n",
    "initial_state, final_state = key.split(\"2\")\n",
    "initial_arr = board.str2arr(initial_state)\n",
    "final_arr = board.str2arr(final_state)\n",
    "\n",
    "print((final_arr - initial_arr).sum())\n",
    "print(final_arr)\n",
    "p_type = (final_arr - initial_arr).sum()\n",
    "Q_final = state[\"Q\"]*p_type\n",
    "print(Q_final)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.26904967]]]], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.stack([[final_arr]],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 7,\n",
       " 9,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 20,\n",
       " 21,\n",
       " 26,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 54,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 64,\n",
       " 66,\n",
       " 69,\n",
       " 74,\n",
       " 77,\n",
       " 82,\n",
       " 84,\n",
       " 85,\n",
       " 87,\n",
       " 93,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 104,\n",
       " 106,\n",
       " 107,\n",
       " 114,\n",
       " 128,\n",
       " 129,\n",
       " 132,\n",
       " 142,\n",
       " 151,\n",
       " 152,\n",
       " 157,\n",
       " 160,\n",
       " 164,\n",
       " 165,\n",
       " 177,\n",
       " 182,\n",
       " 185,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 204,\n",
       " 206,\n",
       " 207,\n",
       " 209,\n",
       " 210,\n",
       " 215,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 227,\n",
       " 229,\n",
       " 232,\n",
       " 239,\n",
       " 242,\n",
       " 244,\n",
       " 246,\n",
       " 249,\n",
       " 266,\n",
       " 270,\n",
       " 275,\n",
       " 291,\n",
       " 293,\n",
       " 294,\n",
       " 299,\n",
       " 300,\n",
       " 309,\n",
       " 311,\n",
       " 312,\n",
       " 315,\n",
       " 316,\n",
       " 330,\n",
       " 334,\n",
       " 335,\n",
       " 340,\n",
       " 343,\n",
       " 346,\n",
       " 348,\n",
       " 349,\n",
       " 354,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 368,\n",
       " 371,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 386,\n",
       " 388,\n",
       " 389,\n",
       " 404,\n",
       " 406,\n",
       " 408,\n",
       " 416,\n",
       " 418,\n",
       " 419,\n",
       " 425,\n",
       " 432,\n",
       " 434,\n",
       " 440,\n",
       " 441,\n",
       " 444,\n",
       " 449,\n",
       " 451,\n",
       " 457,\n",
       " 466,\n",
       " 468,\n",
       " 470,\n",
       " 472,\n",
       " 473,\n",
       " 477,\n",
       " 478,\n",
       " 480,\n",
       " 481,\n",
       " 486,\n",
       " 497,\n",
       " 499,\n",
       " 503,\n",
       " 517,\n",
       " 527,\n",
       " 529,\n",
       " 535,\n",
       " 538,\n",
       " 541,\n",
       " 542,\n",
       " 549,\n",
       " 550,\n",
       " 559,\n",
       " 562,\n",
       " 567,\n",
       " 572,\n",
       " 579,\n",
       " 581,\n",
       " 583,\n",
       " 590,\n",
       " 595,\n",
       " 601,\n",
       " 603,\n",
       " 604,\n",
       " 605,\n",
       " 607,\n",
       " 618,\n",
       " 620,\n",
       " 621,\n",
       " 623,\n",
       " 625,\n",
       " 627,\n",
       " 633,\n",
       " 634,\n",
       " 646,\n",
       " 652,\n",
       " 656,\n",
       " 659,\n",
       " 666,\n",
       " 667,\n",
       " 668,\n",
       " 681,\n",
       " 684,\n",
       " 690,\n",
       " 700,\n",
       " 704,\n",
       " 705,\n",
       " 706,\n",
       " 707,\n",
       " 712,\n",
       " 714,\n",
       " 717,\n",
       " 718,\n",
       " 736,\n",
       " 740,\n",
       " 742,\n",
       " 747,\n",
       " 749,\n",
       " 750,\n",
       " 755,\n",
       " 759,\n",
       " 761,\n",
       " 766,\n",
       " 767,\n",
       " 777,\n",
       " 783,\n",
       " 784,\n",
       " 786,\n",
       " 787,\n",
       " 791,\n",
       " 793,\n",
       " 798,\n",
       " 801,\n",
       " 804,\n",
       " 809,\n",
       " 810,\n",
       " 827,\n",
       " 831,\n",
       " 832,\n",
       " 833,\n",
       " 834,\n",
       " 839,\n",
       " 840,\n",
       " 842,\n",
       " 843,\n",
       " 852,\n",
       " 858,\n",
       " 864,\n",
       " 876,\n",
       " 878,\n",
       " 879,\n",
       " 887,\n",
       " 888,\n",
       " 898,\n",
       " 900,\n",
       " 904,\n",
       " 906,\n",
       " 909,\n",
       " 912,\n",
       " 914,\n",
       " 915,\n",
       " 926,\n",
       " 936,\n",
       " 940,\n",
       " 946,\n",
       " 953,\n",
       " 956,\n",
       " 962,\n",
       " 965,\n",
       " 966,\n",
       " 969,\n",
       " 970,\n",
       " 971,\n",
       " 974,\n",
       " 978,\n",
       " 983,\n",
       " 987,\n",
       " 989,\n",
       " 990,\n",
       " 992,\n",
       " 998,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1013,\n",
       " 1020,\n",
       " 1024,\n",
       " 1032,\n",
       " 1036,\n",
       " 1043,\n",
       " 1045,\n",
       " 1046,\n",
       " 1053,\n",
       " 1054,\n",
       " 1055,\n",
       " 1057,\n",
       " 1062,\n",
       " 1063,\n",
       " 1067,\n",
       " 1072,\n",
       " 1076,\n",
       " 1079,\n",
       " 1085,\n",
       " 1092,\n",
       " 1095,\n",
       " 1101,\n",
       " 1104,\n",
       " 1108,\n",
       " 1109,\n",
       " 1110,\n",
       " 1111,\n",
       " 1113,\n",
       " 1118,\n",
       " 1119,\n",
       " 1120,\n",
       " 1122,\n",
       " 1126,\n",
       " 1127,\n",
       " 1129,\n",
       " 1133,\n",
       " 1134,\n",
       " 1136,\n",
       " 1137,\n",
       " 1148,\n",
       " 1150,\n",
       " 1152,\n",
       " 1155,\n",
       " 1160,\n",
       " 1163,\n",
       " 1166,\n",
       " 1167,\n",
       " 1169,\n",
       " 1170,\n",
       " 1173,\n",
       " 1174,\n",
       " 1179,\n",
       " 1186,\n",
       " 1190,\n",
       " 1193,\n",
       " 1205,\n",
       " 1211,\n",
       " 1212,\n",
       " 1219,\n",
       " 1221,\n",
       " 1224,\n",
       " 1225,\n",
       " 1228,\n",
       " 1229,\n",
       " 1230,\n",
       " 1235,\n",
       " 1239,\n",
       " 1245,\n",
       " 1246,\n",
       " 1247,\n",
       " 1248,\n",
       " 1251,\n",
       " 1253,\n",
       " 1256,\n",
       " 1264,\n",
       " 1268,\n",
       " 1279,\n",
       " 1281,\n",
       " 1284,\n",
       " 1286,\n",
       " 1287,\n",
       " 1292,\n",
       " 1294,\n",
       " 1302,\n",
       " 1304,\n",
       " 1314,\n",
       " 1320,\n",
       " 1321,\n",
       " 1324,\n",
       " 1332,\n",
       " 1335,\n",
       " 1338,\n",
       " 1341,\n",
       " 1349,\n",
       " 1354,\n",
       " 1355,\n",
       " 1362,\n",
       " 1367,\n",
       " 1370,\n",
       " 1372,\n",
       " 1374,\n",
       " 1375,\n",
       " 1376,\n",
       " 1379,\n",
       " 1382,\n",
       " 1385,\n",
       " 1406,\n",
       " 1409,\n",
       " 1410,\n",
       " 1414,\n",
       " 1425,\n",
       " 1431,\n",
       " 1434,\n",
       " 1436,\n",
       " 1437,\n",
       " 1439,\n",
       " 1440,\n",
       " 1442,\n",
       " 1445,\n",
       " 1453,\n",
       " 1456,\n",
       " 1467,\n",
       " 1468,\n",
       " 1476,\n",
       " 1478,\n",
       " 1488,\n",
       " 1489,\n",
       " 1495,\n",
       " 1499,\n",
       " 1502,\n",
       " 1503,\n",
       " 1504,\n",
       " 1505,\n",
       " 1508,\n",
       " 1509,\n",
       " 1512,\n",
       " 1514,\n",
       " 1516,\n",
       " 1520,\n",
       " 1522,\n",
       " 1524,\n",
       " 1528,\n",
       " 1529,\n",
       " 1530,\n",
       " 1531,\n",
       " 1537,\n",
       " 1543,\n",
       " 1547,\n",
       " 1552,\n",
       " 1556,\n",
       " 1563,\n",
       " 1565,\n",
       " 1575,\n",
       " 1576,\n",
       " 1578,\n",
       " 1584,\n",
       " 1585,\n",
       " 1589,\n",
       " 1594,\n",
       " 1608,\n",
       " 1609,\n",
       " 1611,\n",
       " 1614,\n",
       " 1615,\n",
       " 1623,\n",
       " 1630,\n",
       " 1632,\n",
       " 1633,\n",
       " 1635,\n",
       " 1636,\n",
       " 1638,\n",
       " 1650,\n",
       " 1659,\n",
       " 1661,\n",
       " 1664,\n",
       " 1670,\n",
       " 1671,\n",
       " 1673,\n",
       " 1675,\n",
       " 1679,\n",
       " 1680,\n",
       " 1683,\n",
       " 1684,\n",
       " 1687,\n",
       " 1691,\n",
       " 1693,\n",
       " 1697,\n",
       " 1698,\n",
       " 1701,\n",
       " 1702,\n",
       " 1708,\n",
       " 1709,\n",
       " 1710,\n",
       " 1718,\n",
       " 1721,\n",
       " 1729,\n",
       " 1732,\n",
       " 1734,\n",
       " 1740,\n",
       " 1748,\n",
       " 1751,\n",
       " 1754,\n",
       " 1755,\n",
       " 1756,\n",
       " 1757,\n",
       " 1759,\n",
       " 1762,\n",
       " 1769,\n",
       " 1771,\n",
       " 1774,\n",
       " 1775,\n",
       " 1778,\n",
       " 1781,\n",
       " 1785,\n",
       " 1800,\n",
       " 1801,\n",
       " 1804,\n",
       " 1805,\n",
       " 1806,\n",
       " 1812,\n",
       " 1815,\n",
       " 1817,\n",
       " 1818,\n",
       " 1819,\n",
       " 1820,\n",
       " 1821,\n",
       " 1824,\n",
       " 1826,\n",
       " 1835,\n",
       " 1844,\n",
       " 1847,\n",
       " 1851,\n",
       " 1853,\n",
       " 1856,\n",
       " 1861,\n",
       " 1864,\n",
       " 1865,\n",
       " 1870,\n",
       " 1872,\n",
       " 1876,\n",
       " 1880,\n",
       " 1886,\n",
       " 1892,\n",
       " 1900,\n",
       " 1901,\n",
       " 1903,\n",
       " 1907,\n",
       " 1909,\n",
       " 1912,\n",
       " 1914,\n",
       " 1915,\n",
       " 1916,\n",
       " 1920,\n",
       " 1929,\n",
       " 1934,\n",
       " 1937,\n",
       " 1941,\n",
       " 1946,\n",
       " 1948,\n",
       " 1951,\n",
       " 1952,\n",
       " 1955,\n",
       " 1957,\n",
       " 1958,\n",
       " 1961,\n",
       " 1963,\n",
       " 1964,\n",
       " 1974,\n",
       " 1975,\n",
       " 1979,\n",
       " 1981,\n",
       " 1983,\n",
       " 1984,\n",
       " 1985,\n",
       " 1988,\n",
       " 1992,\n",
       " 2008,\n",
       " 2014,\n",
       " 2021,\n",
       " 2027,\n",
       " 2028,\n",
       " 2029,\n",
       " 2037,\n",
       " 2040,\n",
       " 2046,\n",
       " 2052,\n",
       " 2062,\n",
       " 2067,\n",
       " 2068,\n",
       " 2073,\n",
       " 2079,\n",
       " 2090,\n",
       " 2092,\n",
       " 2099,\n",
       " 2101,\n",
       " 2104,\n",
       " 2107,\n",
       " 2111,\n",
       " 2117,\n",
       " 2118,\n",
       " 2121,\n",
       " 2123,\n",
       " 2127,\n",
       " 2128,\n",
       " 2131,\n",
       " 2133,\n",
       " 2134,\n",
       " 2141,\n",
       " 2147,\n",
       " 2148,\n",
       " 2152,\n",
       " 2156,\n",
       " 2158,\n",
       " 2160,\n",
       " 2164,\n",
       " 2166,\n",
       " 2174,\n",
       " 2180,\n",
       " 2181,\n",
       " 2182,\n",
       " 2189,\n",
       " 2193,\n",
       " 2198,\n",
       " 2201,\n",
       " 2203,\n",
       " 2208,\n",
       " 2209,\n",
       " 2210,\n",
       " 2211,\n",
       " 2216,\n",
       " 2217,\n",
       " 2221,\n",
       " 2223,\n",
       " 2232,\n",
       " 2233,\n",
       " 2235,\n",
       " 2236,\n",
       " 2237,\n",
       " 2238,\n",
       " 2242,\n",
       " 2243,\n",
       " 2244,\n",
       " 2252,\n",
       " 2253,\n",
       " 2254,\n",
       " 2255,\n",
       " 2258,\n",
       " 2260,\n",
       " 2261,\n",
       " 2262,\n",
       " 2266,\n",
       " 2268,\n",
       " 2271,\n",
       " 2276,\n",
       " 2281,\n",
       " 2288,\n",
       " 2289,\n",
       " 2291,\n",
       " 2293,\n",
       " 2295,\n",
       " 2300,\n",
       " 2304,\n",
       " 2306,\n",
       " 2308,\n",
       " 2311,\n",
       " 2317,\n",
       " 2322,\n",
       " 2325,\n",
       " 2326,\n",
       " 2327,\n",
       " 2331,\n",
       " 2337,\n",
       " 2338,\n",
       " 2347,\n",
       " 2355,\n",
       " 2356,\n",
       " 2357,\n",
       " 2358,\n",
       " 2359,\n",
       " 2360,\n",
       " 2362,\n",
       " 2364,\n",
       " 2367,\n",
       " 2368,\n",
       " 2375,\n",
       " 2379,\n",
       " 2382,\n",
       " 2386,\n",
       " 2387,\n",
       " 2388,\n",
       " 2394,\n",
       " 2398,\n",
       " 2399,\n",
       " 2407,\n",
       " 2413,\n",
       " 2414,\n",
       " 2418,\n",
       " 2423,\n",
       " 2428,\n",
       " 2436,\n",
       " 2439,\n",
       " 2440,\n",
       " 2444,\n",
       " 2445,\n",
       " 2446,\n",
       " 2448,\n",
       " 2455,\n",
       " 2457,\n",
       " 2458,\n",
       " 2460,\n",
       " 2467,\n",
       " 2468,\n",
       " 2469,\n",
       " 2474,\n",
       " 2475,\n",
       " 2484,\n",
       " 2485,\n",
       " 2492,\n",
       " 2494,\n",
       " 2495,\n",
       " 2498,\n",
       " 2499,\n",
       " 2504,\n",
       " 2510,\n",
       " 2516,\n",
       " 2523,\n",
       " 2527,\n",
       " 2528,\n",
       " 2530,\n",
       " 2531,\n",
       " 2532,\n",
       " 2543,\n",
       " 2554,\n",
       " 2563,\n",
       " 2564,\n",
       " 2568,\n",
       " 2573,\n",
       " 2574,\n",
       " 2576,\n",
       " 2585,\n",
       " 2596,\n",
       " 2602,\n",
       " 2603,\n",
       " 2604,\n",
       " 2605,\n",
       " 2610,\n",
       " 2622,\n",
       " 2624,\n",
       " 2627,\n",
       " 2628,\n",
       " 2629,\n",
       " 2644,\n",
       " 2646,\n",
       " 2651,\n",
       " 2662,\n",
       " 2664,\n",
       " 2668,\n",
       " 2669,\n",
       " 2673,\n",
       " 2675,\n",
       " 2680,\n",
       " 2682,\n",
       " 2686,\n",
       " 2687,\n",
       " 2699,\n",
       " 2703,\n",
       " 2704,\n",
       " 2716,\n",
       " 2717,\n",
       " 2721,\n",
       " 2723,\n",
       " 2726,\n",
       " 2727,\n",
       " 2734,\n",
       " 2735,\n",
       " 2739,\n",
       " 2742,\n",
       " 2746,\n",
       " 2748,\n",
       " 2751,\n",
       " 2752,\n",
       " 2758,\n",
       " 2759,\n",
       " 2765,\n",
       " 2766,\n",
       " 2768,\n",
       " 2778,\n",
       " 2779,\n",
       " 2780,\n",
       " 2784,\n",
       " 2787,\n",
       " 2790,\n",
       " 2792,\n",
       " 2793,\n",
       " 2798,\n",
       " 2803,\n",
       " 2804,\n",
       " 2809,\n",
       " 2821,\n",
       " 2831,\n",
       " 2835,\n",
       " 2841,\n",
       " 2842,\n",
       " 2845,\n",
       " 2846,\n",
       " 2849,\n",
       " 2851,\n",
       " 2854,\n",
       " 2862,\n",
       " 2863,\n",
       " 2864,\n",
       " 2865,\n",
       " 2868,\n",
       " 2871,\n",
       " 2876,\n",
       " 2883,\n",
       " 2885,\n",
       " 2889,\n",
       " 2892,\n",
       " 2895,\n",
       " 2897,\n",
       " 2898,\n",
       " 2902,\n",
       " 2903,\n",
       " 2904,\n",
       " 2906,\n",
       " 2910,\n",
       " 2913,\n",
       " 2918,\n",
       " 2921,\n",
       " 2932,\n",
       " 2933,\n",
       " 2939,\n",
       " 2948,\n",
       " 2950,\n",
       " 2951,\n",
       " 2956,\n",
       " 2958,\n",
       " 2960,\n",
       " 2968,\n",
       " 2971,\n",
       " 2972,\n",
       " 2977,\n",
       " 2981,\n",
       " 2987,\n",
       " 2989,\n",
       " 2990,\n",
       " 2995,\n",
       " 2996,\n",
       " 2998,\n",
       " 3010,\n",
       " 3016,\n",
       " 3021,\n",
       " 3024,\n",
       " 3025,\n",
       " 3028,\n",
       " 3030,\n",
       " 3036,\n",
       " 3040,\n",
       " 3044,\n",
       " 3045,\n",
       " 3048,\n",
       " 3051,\n",
       " 3054,\n",
       " 3056,\n",
       " 3058,\n",
       " 3060,\n",
       " 3061,\n",
       " 3066,\n",
       " 3068,\n",
       " 3069,\n",
       " 3071,\n",
       " 3073,\n",
       " 3074,\n",
       " 3075,\n",
       " 3078,\n",
       " 3080,\n",
       " 3084,\n",
       " 3094,\n",
       " 3095,\n",
       " 3103,\n",
       " 3114,\n",
       " 3115,\n",
       " 3117,\n",
       " 3125,\n",
       " 3131,\n",
       " 3137,\n",
       " 3138,\n",
       " 3139,\n",
       " 3140,\n",
       " 3142,\n",
       " 3143,\n",
       " 3147,\n",
       " 3151,\n",
       " 3152,\n",
       " 3162,\n",
       " 3165,\n",
       " 3168,\n",
       " 3172,\n",
       " 3176,\n",
       " 3178,\n",
       " 3179,\n",
       " 3181,\n",
       " 3183,\n",
       " 3188,\n",
       " 3190,\n",
       " 3197,\n",
       " 3203,\n",
       " 3206,\n",
       " 3208,\n",
       " 3209,\n",
       " 3211,\n",
       " 3212,\n",
       " 3217,\n",
       " 3220,\n",
       " 3223,\n",
       " 3242,\n",
       " 3243,\n",
       " 3245,\n",
       " 3259,\n",
       " 3263,\n",
       " 3272,\n",
       " 3285,\n",
       " 3286,\n",
       " 3288,\n",
       " 3289,\n",
       " 3292,\n",
       " 3294,\n",
       " 3296,\n",
       " 3298,\n",
       " 3300,\n",
       " 3303,\n",
       " 3306,\n",
       " 3311,\n",
       " 3312,\n",
       " 3314,\n",
       " 3317,\n",
       " 3320,\n",
       " 3322,\n",
       " 3323,\n",
       " 3326,\n",
       " 3329,\n",
       " 3330,\n",
       " 3331,\n",
       " 3335,\n",
       " 3337,\n",
       " 3339,\n",
       " 3340,\n",
       " 3346,\n",
       " 3347,\n",
       " 3348,\n",
       " 3350,\n",
       " 3352,\n",
       " 3363,\n",
       " 3364,\n",
       " 3375,\n",
       " 3377,\n",
       " 3381,\n",
       " 3383,\n",
       " 3384,\n",
       " 3385,\n",
       " 3393,\n",
       " 3394,\n",
       " 3402,\n",
       " 3416,\n",
       " 3418,\n",
       " 3419,\n",
       " 3427,\n",
       " 3431,\n",
       " 3433,\n",
       " 3440,\n",
       " 3450,\n",
       " 3453,\n",
       " 3455,\n",
       " 3458,\n",
       " 3464,\n",
       " 3465,\n",
       " 3469,\n",
       " 3474,\n",
       " 3479,\n",
       " 3481,\n",
       " 3486,\n",
       " 3487,\n",
       " 3491,\n",
       " 3493,\n",
       " 3494,\n",
       " 3497,\n",
       " 3499,\n",
       " 3505,\n",
       " 3506,\n",
       " 3508,\n",
       " 3513,\n",
       " 3516,\n",
       " 3524,\n",
       " 3525,\n",
       " 3527,\n",
       " 3528,\n",
       " 3530,\n",
       " 3531,\n",
       " 3533,\n",
       " 3538,\n",
       " 3545,\n",
       " 3551,\n",
       " 3553,\n",
       " 3554,\n",
       " 3556,\n",
       " 3561,\n",
       " 3567,\n",
       " 3578,\n",
       " 3579,\n",
       " 3581,\n",
       " 3582,\n",
       " 3583,\n",
       " 3586,\n",
       " 3592,\n",
       " 3596,\n",
       " 3600,\n",
       " 3601,\n",
       " 3604,\n",
       " 3605,\n",
       " 3610,\n",
       " 3613,\n",
       " 3615,\n",
       " 3618,\n",
       " 3621,\n",
       " 3622,\n",
       " 3644,\n",
       " 3645,\n",
       " 3646,\n",
       " 3647,\n",
       " 3648,\n",
       " 3658,\n",
       " 3662,\n",
       " 3666,\n",
       " 3669,\n",
       " 3678,\n",
       " 3681,\n",
       " 3686,\n",
       " 3692,\n",
       " 3696,\n",
       " 3699,\n",
       " 3710,\n",
       " 3717,\n",
       " ...]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i,y in enumerate(Y) if y ==1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
