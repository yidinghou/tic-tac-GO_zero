{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/yidinghou/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import player\n",
    "import game\n",
    "import neural_network\n",
    "import mcts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import board as b\n",
    "import keras\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16167"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree, edge_statistics = mcts.MCTS.get_tree_and_edges()\n",
    "board = b.Board()\n",
    "\n",
    "len(edge_statistics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "Y_move = []\n",
    "\n",
    "keys = list(edge_statistics.keys())\n",
    "for key in keys:\n",
    "    state = edge_statistics[key]\n",
    "    initial_state, final_state = key.split(\"2\")\n",
    "    initial_arr = board.str2arr(initial_state)\n",
    "    final_arr = board.str2arr(final_state)\n",
    "    move = final_arr - initial_arr\n",
    "    p_type = move.sum()\n",
    "    move = move*p_type\n",
    "    move = move.reshape(-1, 9)[0]\n",
    "    \n",
    "    X.append(initial_arr)\n",
    "    Y.append(state[\"Q\"]*p_type)\n",
    "    Y_move.append(move)\n",
    "    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y_move = np.array(Y_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos = [i for i,y in enumerate(Y) if y ==1]\n",
    "Y_neg = [i for i,y in enumerate(Y) if y ==-1]\n",
    "Y_drw = [i for i,y in enumerate(Y) if y ==0]\n",
    "\n",
    "Y_clean = np.concatenate([Y[Y_pos], \n",
    "                          Y[Y_neg],\n",
    "                          Y[Y_drw],\n",
    "                         ])\n",
    "\n",
    "Y_policy_clean = np.concatenate([Y_move[Y_pos], \n",
    "                                 Y_move[Y_neg],\n",
    "                                 Y_move[Y_drw]\n",
    "                                ])\n",
    "\n",
    "\n",
    "X_clean = np.concatenate([X[Y_pos], \n",
    "                          X[Y_neg],\n",
    "                          X[Y_drw],\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "nb_classes = 3\n",
    "targets = (Y_clean+1)\n",
    "one_hot_targets = np.eye(nb_classes)[targets.astype(int)]\n",
    "one_hot_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 3, 3, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 3, 1, 3)      12          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 1, 3, 3)      12          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 1, 1, 8)      80          input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling2D) (None, 1, 1, 3)      0           conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_49 (MaxPooling2D) (None, 1, 1, 3)      0           conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling2D) (None, 1, 1, 8)      0           conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 1, 1, 14)     0           max_pooling2d_48[0][0]           \n",
      "                                                                 max_pooling2d_49[0][0]           \n",
      "                                                                 max_pooling2d_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 14)           0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 10)           150         flatten_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "V (Dense)                       (None, 1)            11          dense_31[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 265\n",
      "Trainable params: 265\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv1D, Flatten, Conv2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "\n",
    "Input_1= Input(shape=(3,3,1))\n",
    "\n",
    "x1 = Conv2D(filters = 3, kernel_size=(1,3), activation='relu', \n",
    "            kernel_regularizer=l2(0.0005),\n",
    "            kernel_initializer=initializers.RandomNormal(stddev=0.1, mean=0),\n",
    "            input_shape=(3,3,1))(Input_1)\n",
    "\n",
    "x2 = Conv2D(filters = 3, kernel_size=(3,1), activation='relu', \n",
    "            kernel_regularizer=l2(0.0005),\n",
    "            kernel_initializer=initializers.RandomNormal(stddev=0.1, mean=0),\n",
    "            input_shape=(3,3,1))(Input_1)\n",
    "\n",
    "x3 = Conv2D(filters = 8, kernel_size=(3,3), activation='relu', \n",
    "            kernel_regularizer=l2(0.0005),\n",
    "            kernel_initializer=initializers.RandomNormal(stddev=0.1, mean=0),\n",
    "            input_shape=(3,3,1))(Input_1)\n",
    "\n",
    "x1 = MaxPooling2D((3, 1))(x1)\n",
    "x2 = MaxPooling2D((1, 3))(x2)\n",
    "x3 = MaxPooling2D((1,1))(x3)\n",
    "\n",
    "\n",
    "x = Concatenate()([x1, x2, x3])\n",
    "# x = MaxPooling2D((3,1))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "value_head = Dense(10,  activation='relu')(x)\n",
    "# value_head = Dense(10,  activation='relu')(x)\n",
    "# value_head = Dense(3,  activation='softmax', name = \"V\")(value_head)\n",
    "value_head = Dense(1,  activation='relu', name = \"V\")(value_head)\n",
    "\n",
    "policy_head = Dense(90,  activation='relu')(x)\n",
    "# policy_head = Dense(10,  activation='relu')(policy_head)\n",
    "policy_head = Dense(9,  activation='softmax', name = \"P\")(policy_head)\n",
    "\n",
    "# model = Model(inputs=Input_1, outputs=[value_head, policy_head])\n",
    "model = Model(inputs=Input_1, outputs=[value_head])\n",
    "# model = Model(inputs=Input_1, outputs=[policy_head])\n",
    "\n",
    "opt = SGD(lr=0.01, momentum=0.09)\n",
    "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                             \n",
    "# model.compile(optimizer=opt, \n",
    "#               loss={'V':'mse', \"P\": \"categorical_crossentropy\"},\n",
    "#               loss_weights = {\"V\": 1.0, \"P\": .4},\n",
    "#               metrics=['acc'])\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss=\"mse\",\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pos = [i for i,y in enumerate(Y_clean) if y ==1]\n",
    "Y_neg = [i for i,y in enumerate(Y_clean) if y ==-1]\n",
    "Y_drw = [i for i,y in enumerate(Y_clean) if y ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 62us/sample - loss: 0.1324 - acc: 0.4880\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1309 - acc: 0.4917\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1303 - acc: 0.4917\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1299 - acc: 0.4907\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1297 - acc: 0.4917\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 79us/sample - loss: 0.1294 - acc: 0.4910\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.1293 - acc: 0.4907\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.1289 - acc: 0.4940\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.1287 - acc: 0.4903\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1286 - acc: 0.4940\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1331 - acc: 0.4793\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1324 - acc: 0.4860\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1318 - acc: 0.4810\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1313 - acc: 0.4873\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1310 - acc: 0.4873\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1305 - acc: 0.4913\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1303 - acc: 0.4887\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.1301 - acc: 0.4913\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1300 - acc: 0.4947\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.1299 - acc: 0.4940\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.1336 - acc: 0.4927\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1322 - acc: 0.4973\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1316 - acc: 0.4923\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.1309 - acc: 0.4987\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.1303 - acc: 0.4997\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.1298 - acc: 0.5020\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.1293 - acc: 0.5033\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.1293 - acc: 0.5013\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1289 - acc: 0.5043\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1289 - acc: 0.5027\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.1365 - acc: 0.4763\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.1355 - acc: 0.4777\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1346 - acc: 0.4773\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 48us/sample - loss: 0.1341 - acc: 0.4843\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1340 - acc: 0.4810\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1339 - acc: 0.4823\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.1333 - acc: 0.4813\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.1331 - acc: 0.4857\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1329 - acc: 0.4847\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1327 - acc: 0.4860\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.1319 - acc: 0.4873\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1311 - acc: 0.4880\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1305 - acc: 0.4860\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1302 - acc: 0.4880\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 52us/sample - loss: 0.1293 - acc: 0.4947\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.1296 - acc: 0.4920\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1289 - acc: 0.4957\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1289 - acc: 0.4940\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1286 - acc: 0.4947\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1284 - acc: 0.4980\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1312 - acc: 0.4937\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1300 - acc: 0.4950\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1298 - acc: 0.4950\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1294 - acc: 0.4947\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.1291 - acc: 0.4943\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1288 - acc: 0.4957\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1285 - acc: 0.4940\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1286 - acc: 0.4937\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1283 - acc: 0.4990\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1283 - acc: 0.4997\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1298 - acc: 0.4900\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1286 - acc: 0.4933\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.1279 - acc: 0.4933\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.1274 - acc: 0.4940\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1272 - acc: 0.4970\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1266 - acc: 0.4997\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1264 - acc: 0.5020\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1265 - acc: 0.4987\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1263 - acc: 0.4990\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1260 - acc: 0.4990\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1304 - acc: 0.4833\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1297 - acc: 0.4853\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1290 - acc: 0.4873\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1282 - acc: 0.4910\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1279 - acc: 0.4937\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 45us/sample - loss: 0.1278 - acc: 0.4890\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1273 - acc: 0.4933\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1271 - acc: 0.4967\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1268 - acc: 0.4947\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 43us/sample - loss: 0.1269 - acc: 0.4960\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 46us/sample - loss: 0.1320 - acc: 0.4913\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1305 - acc: 0.4957\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1297 - acc: 0.4950\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1295 - acc: 0.4923\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 32us/sample - loss: 0.1289 - acc: 0.4923\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1288 - acc: 0.4923\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1285 - acc: 0.4920\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1282 - acc: 0.4967\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1280 - acc: 0.4950\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1276 - acc: 0.4930\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.1311 - acc: 0.4883\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1297 - acc: 0.4900\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1292 - acc: 0.4933\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1287 - acc: 0.4940\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1287 - acc: 0.4970\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1278 - acc: 0.5003\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1285 - acc: 0.4940\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1277 - acc: 0.5000\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1279 - acc: 0.5020\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1279 - acc: 0.4973\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1300 - acc: 0.4977\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1290 - acc: 0.4990\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1283 - acc: 0.5037\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1281 - acc: 0.5017\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1276 - acc: 0.5017\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1274 - acc: 0.5040\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1272 - acc: 0.5017\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1268 - acc: 0.4997\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1269 - acc: 0.5013\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1266 - acc: 0.5040\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1317 - acc: 0.4840\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1304 - acc: 0.4867\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1296 - acc: 0.4887\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1295 - acc: 0.4917\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1290 - acc: 0.4960\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1284 - acc: 0.4947\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1284 - acc: 0.4930\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1282 - acc: 0.4903\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1277 - acc: 0.4940\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.1276 - acc: 0.4920\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1271 - acc: 0.5030\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1255 - acc: 0.5043\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1244 - acc: 0.5077\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1242 - acc: 0.5070\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1240 - acc: 0.5097\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1234 - acc: 0.5080\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1232 - acc: 0.5077\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1230 - acc: 0.5087\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1229 - acc: 0.5097\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1229 - acc: 0.5090\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1275 - acc: 0.5057\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1260 - acc: 0.5070\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1251 - acc: 0.5100\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1248 - acc: 0.5103\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1244 - acc: 0.5073\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1242 - acc: 0.5087\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1241 - acc: 0.5080\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1238 - acc: 0.5080\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1236 - acc: 0.5130\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1237 - acc: 0.5107\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1331 - acc: 0.4847\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1315 - acc: 0.4873\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1307 - acc: 0.4943\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1299 - acc: 0.4923\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1298 - acc: 0.4923\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1296 - acc: 0.4953\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1291 - acc: 0.4953\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1289 - acc: 0.4950\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1284 - acc: 0.4933\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1283 - acc: 0.5003\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1240 - acc: 0.5023\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1228 - acc: 0.5070\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1219 - acc: 0.5090\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1214 - acc: 0.5077\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1207 - acc: 0.5130\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1207 - acc: 0.5137\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1202 - acc: 0.5127\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1204 - acc: 0.5127\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1200 - acc: 0.5133\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1198 - acc: 0.5130\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1255 - acc: 0.4957\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1244 - acc: 0.4947\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1237 - acc: 0.4963\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1233 - acc: 0.4967\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1228 - acc: 0.5053\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1227 - acc: 0.5020\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1223 - acc: 0.5040\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1222 - acc: 0.5033\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1221 - acc: 0.5043\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1220 - acc: 0.5023\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1284 - acc: 0.4990\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1272 - acc: 0.4997\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1263 - acc: 0.5037\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1257 - acc: 0.5047\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1257 - acc: 0.5043\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1252 - acc: 0.5077\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1254 - acc: 0.5053\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1248 - acc: 0.5057\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1252 - acc: 0.5043\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1249 - acc: 0.5093\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1280 - acc: 0.4957\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1262 - acc: 0.5067\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1257 - acc: 0.5010\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1254 - acc: 0.5057\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1247 - acc: 0.5017\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1248 - acc: 0.5023\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1243 - acc: 0.5077\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1242 - acc: 0.5050\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1241 - acc: 0.5047\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1238 - acc: 0.5067\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1272 - acc: 0.4980\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1255 - acc: 0.4977\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1248 - acc: 0.5057\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1245 - acc: 0.5037\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1242 - acc: 0.5053\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1239 - acc: 0.5053\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1235 - acc: 0.5063\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1234 - acc: 0.5107\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1234 - acc: 0.5063\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1229 - acc: 0.5103\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1261 - acc: 0.4963\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1250 - acc: 0.5050\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1244 - acc: 0.5050\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1237 - acc: 0.5037\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1235 - acc: 0.5037\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1233 - acc: 0.5047\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1231 - acc: 0.5067\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1230 - acc: 0.5100\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1225 - acc: 0.5050\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1226 - acc: 0.5090\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1264 - acc: 0.5017\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1250 - acc: 0.5017\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1242 - acc: 0.5047\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1237 - acc: 0.5047\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1232 - acc: 0.5067\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1231 - acc: 0.5093\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1227 - acc: 0.5097\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1226 - acc: 0.5100\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1224 - acc: 0.5080\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1223 - acc: 0.5103\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1260 - acc: 0.5040\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1249 - acc: 0.5100\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1244 - acc: 0.5037\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1243 - acc: 0.5027\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 36us/sample - loss: 0.1241 - acc: 0.5050\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1239 - acc: 0.5073\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1234 - acc: 0.5043\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1236 - acc: 0.5083\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 35us/sample - loss: 0.1233 - acc: 0.5073\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 34us/sample - loss: 0.1232 - acc: 0.5090\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1285 - acc: 0.4863\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1275 - acc: 0.4910\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 33us/sample - loss: 0.1267 - acc: 0.4907\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1264 - acc: 0.4920\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.1259 - acc: 0.4967\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 57us/sample - loss: 0.1259 - acc: 0.4960\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.1256 - acc: 0.4957\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.1253 - acc: 0.4980\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.1253 - acc: 0.4993\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.1252 - acc: 0.4970\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.1286 - acc: 0.4927\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 59us/sample - loss: 0.1276 - acc: 0.4957\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.1272 - acc: 0.5027\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1269 - acc: 0.4983\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 56us/sample - loss: 0.1269 - acc: 0.5037\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 55us/sample - loss: 0.1266 - acc: 0.5020\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.1260 - acc: 0.5037\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.1255 - acc: 0.5063\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 41us/sample - loss: 0.1260 - acc: 0.5020\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1257 - acc: 0.5093\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 39us/sample - loss: 0.1228 - acc: 0.5070\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 54us/sample - loss: 0.1213 - acc: 0.5127\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 51us/sample - loss: 0.1205 - acc: 0.5123\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1198 - acc: 0.5157\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 47us/sample - loss: 0.1196 - acc: 0.5150\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 38us/sample - loss: 0.1195 - acc: 0.5133\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1190 - acc: 0.5167\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.1192 - acc: 0.5140\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1191 - acc: 0.5153\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1188 - acc: 0.5173\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.1268 - acc: 0.5007\n",
      "Epoch 2/10\n",
      "3000/3000 [==============================] - 0s 53us/sample - loss: 0.1254 - acc: 0.5053\n",
      "Epoch 3/10\n",
      "3000/3000 [==============================] - 0s 50us/sample - loss: 0.1246 - acc: 0.5050\n",
      "Epoch 4/10\n",
      "3000/3000 [==============================] - 0s 37us/sample - loss: 0.1240 - acc: 0.5040\n",
      "Epoch 5/10\n",
      "3000/3000 [==============================] - 0s 40us/sample - loss: 0.1236 - acc: 0.5043\n",
      "Epoch 6/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1234 - acc: 0.5060\n",
      "Epoch 7/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1231 - acc: 0.5097\n",
      "Epoch 8/10\n",
      "3000/3000 [==============================] - 0s 44us/sample - loss: 0.1227 - acc: 0.5073\n",
      "Epoch 9/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1227 - acc: 0.5030\n",
      "Epoch 10/10\n",
      "3000/3000 [==============================] - 0s 49us/sample - loss: 0.1226 - acc: 0.5067\n",
      "Train on 3000 samples\n",
      "Epoch 1/10\n",
      "3000/3000 [==============================] - 0s 42us/sample - loss: 0.1250 - acc: 0.5027\n",
      "Epoch 2/10\n",
      "2208/3000 [=====================>........] - ETA: 0s - loss: 0.1251 - acc: 0.5023"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    pos_sample = np.random.choice(Y_pos, 1000)\n",
    "    neg_sample = np.random.choice(Y_neg, 1000)\n",
    "    drw_sample = np.random.choice(Y_drw, 1000)\n",
    "\n",
    "\n",
    "    X_final = np.concatenate((X_clean[pos_sample], \n",
    "                              X_clean[neg_sample],\n",
    "                              X_clean[drw_sample]\n",
    "                             )) \n",
    "    \n",
    "    Y_final = np.concatenate((Y_clean[pos_sample], \n",
    "                              Y_clean[neg_sample],\n",
    "                              Y_clean[drw_sample]\n",
    "                             )) \n",
    "    Y_final = (Y_final+1)/2\n",
    "\n",
    "\n",
    "    Y_policy_final = np.concatenate((Y_policy_clean[pos_sample], \n",
    "                              Y_policy_clean[neg_sample],\n",
    "                              Y_policy_clean[drw_sample]\n",
    "                             )) \n",
    "#     Y_final = np.concatenate((one_hot_targets[pos_sample], \n",
    "#                           one_hot_targets[neg_sample],\n",
    "#                           one_hot_targets[drw_sample]\n",
    "#                          )) \n",
    "        \n",
    "\n",
    "    \n",
    "    X_inp = np.stack([X_final], axis=-1)\n",
    "    model.fit(X_inp, [Y_final], epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./best_keras_model.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = keras.models.load_model('./best_keras_model.tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_inp = np.stack([X], axis=-1)\n",
    "model.fit(X_inp, Y_norm, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAAyCAYAAABiUAEEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAABt0lEQVR4nO3YMYpTURTH4fNEUAnKhEmKAX1iJdhZpxMX4EZ0C3YiWLgGt+AKBImljFilnDQWprQYZODayxR54Y5/0e8rH49zLlz4FXdorRUAf9619AEA/lcCDBAiwAAhAgwQIsAAIden/Hzj9lGbHZ9c1Vn2cm9+K7q/qurrZhvd337+qHZxPvSat1gs2jiOvcZxoO12W7vdrt+93pm1cTnvNe4gp9/Oo/urqh4/vJs+Qn0+/bJrrS1//z4pwLPjk3r68l2/Ux3g7bNH0f1VVQ+evIjuv9i87zpvHMdar9ddZzLdarXqOm9czuvj6+ddZ041f7WJ7q+q+vThTfoIdfNocXbZd08QACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACFDa23/n4fhe1WdXd1x2NP91tqy1zD3+tdwr/+uS+92UoAB6McTBECIAAOECDBAiAADhAgwQIgAA4QIMECIAAOECDBAyC/eDUQ9685b6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.37722564,  1.1339668 , -0.04477416]], dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = model.layers[1].get_weights()[0]\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "n_filters, ix = 3, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(1):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='RdBu', vmin = -1, vmax = 1)\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "f[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARMAAABRCAYAAADxTkJrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAACD0lEQVR4nO3csWoUURiG4TMSy4VFZiEmMpvGC7BS3M7C0juwtbUWbb0JW+/ASrBeUSxSpzMDK0IGCbFdOBaxdMGDHwbC87Qz/Pxw4OVMM12ttQD8qxtXvQBwPYgJECEmQISYABFiAkTstbzcz2d1ub+ILnD6/axM5z+76FCa9H1fh2GIzhzHsUzT5FyvUH9rXo8OD6Izv26+lenH+R/PtSkmy/1F+fjmdWar3x4+exmdR7thGMp6vY7OXK1W0Xm0Ozo8KJ/fvY3OvP/k6c5nPnOACDEBIsQEiBATIEJMgAgxASLEBIgQEyBCTIAIMQEixASIEBMgQkyACDEBIsQEiBATIEJMgAgxASLEBIho+gfs8XhRZs8/RBfYjhfRecClunezbOd34jN3cTMBIsQEiBATIEJMgAgxASLEBIgQEyBCTIAIMQEixASIEBMgQkyACDEBIsQEiBATIEJMgAgxASLEBIgQEyBCTIAIMQEimv5Of+/u7fLp/avoAg8ef4nOAy4dn2zK7NGL6MztyWbnMzcTIEJMgAgxASLEBIgQEyBCTIAIMQEixASIEBMgQkyACDEBIsQEiBATIEJMgAgxASLEBIgQEyBCTIAIMQEixASI6Gqtf/9y152VUk7DOyxrrYvwTBo41+vpf59rU0wAdvGZA0SICRAhJkCEmAARYgJEiAkQISZAhJgAEWICRPwCgyhLPzfOJEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.17424254],\n",
       "       [-0.05759431],\n",
       "       [ 1.012904  ]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = model.layers[2].get_weights()[0]\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "n_filters, ix = 3, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(1):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='RdBu', vmin = -1, vmax = 1)\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "f[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAABgCAYAAAAzduYkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAFD0lEQVR4nO3dT2gcZRjH8Wd2t/mz+bM2u0nbpJmgoqgRQnqw1UgNtRSChwj15sWLJwWV3hrQihUPoqIHRQ9CT+JJLyLFEGw1lh6kRJqLgphsYmqajUm2sfm3O148eOjzLLN5wuL6/Vx/+05emOyPHeadeYMoigQAvCRqPQEA9YVSAeCKUgHgilIB4IpSAeCKUgHgKhXnw7lMa9TXlVXza/Pr5vjB+3vUbCafl6XCchBnPvCRy+WiMAyrHm8tSpidnZXC0hLntQaagmTUZnzFw8GHzPHT80U1215blNLttTue11il0teVlckPzqp5+9mr5vgr4+fV7NEnR+JMBY7CMJTJyUk1L1dYyrRjfGD4+OPVTgu71CYpOS2H1Pz9SxPm+IGxy2r22+dn1IzLHwCuKBUArigVAK4oFQCuKBUArmLd/ZGWuyRxdFSNb3/xiD2++IcaBeXtWFOBn/y1aXkl/aCajxWum+Nfu/izfuyVjarnhd3pHeyX94y7esXtsjl+6vVjavbYdy1qxi8VAK4oFQCuKBUArigVAK4oFQCuKBUArigVAK7irVMREYn0e9s3LnxkDu184VX9sIn4U4GPSusZEpu3zPEfPq2vcZl6u6nqeWF3tsqR5Iv6+q8Dafs7t5lsV7NykFQzfqkAcEWpAHBFqQBwRakAcEWpAHBFqQBwRakAcBVEUYVXpf/7w0FwU0Rm9mgufVEUde7RsWHgvNanWp3XWKUCAJVw+QPAFaUCwFWsB24yHdnoYE+vmm/u2O+8bErpHXZjPi8rywW2x6yB5iAZZYx/hZ6BB+wDJPTnQGZmZ2WJbU9roiObiw736tvZLkxNm+MPDfSr2Vx+VpYLdz6vsUrlYE+vfPzluJr/uvyXOf6+bFrNnh89GWcqcJSRlDyb7FbztyYumuPLzRk1Gxoaqnpe2J3DvaF8NaFvXfpm9mFz/Jgx9qkTx9WMyx8ArigVAK4oFQCuKBUArigVAK4oFQCuYt1STgQi6X16D3W1NJjjc2k9TyVYylArnQP98uL4JTVfb9DXoYiILKxuqdlmicdAamWhuCFvfPOLmr+8MGWOTxpfycDI+KUCwBWlAsAVpQLAFaUCwBWlAsAVpQLAFaUCwFWsdSoNyYSE7Y1q/mnfEXP8yPUrarZR4V0s2DuJQKTZeNdN69qcOf6e/fo7dhqtxQ7YUwdaG+XM8L1qnmm01x9tl/U1RtZbaPmlAsAVpQLAFaUCwBWlAsAVpQLAFaUCwBWlAsAVeymD81qn2EsZQF3g8geAK0oFgKtYz/7kcrkoDPW9WYOS/q7Sfz6hJjNz87JUWOZBkRrY35GNuo09d0vGMyAiIiXjEnpxfk5W/2SP7P+TWKUShqFc/n5SzRtX8vYBkvqfO3ZqNM5U4Ki7N5TPvv5WzYubO+b4VSN/6ZlT1U4L/1Fc/gBwRakAcEWpAHBFqQBwRakAcBXr7k+58LtsXTin5h2f3DTHF98Z1sONW3GmAkfrWyW5Orei5j8efcIcf/KnH9Sswt1o1CF+qQBwRakAcEWpAHBFqQBwRakAcEWpAHBFqQBwFWudSiLbLc3PnVPzxX3n7QMcGdGz9LtxpgJHbY0pOXF3h5oPLUyZ4wPjlRbWdqqoT5xxAK4oFQCuKBUArigVAK4oFQCuKBUArigVAK7Y9hScV7hi21MArrj8AeCKUgHgilIB4IpSAeCKUgHgilIB4IpSAeCKUgHgilIB4OpvecYmWl5AoQQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.19671421,  0.23649517,  0.28602538],\n",
       "       [ 0.36910164, -1.0180085 ,  0.31916684],\n",
       "       [ 0.13662557,  0.17839466,  0.16965307]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters = model.layers[3].get_weights()[0]\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "n_filters, ix = 8, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(1):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        plt.imshow(f[:, :, j], cmap='RdBu', vmin = -1, vmax = 1)\n",
    "        ix += 1\n",
    "\n",
    "# show the figure\n",
    "plt.show()\n",
    "f[:, :, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "[[-1  0  0]\n",
      " [-1  0  1]\n",
      " [ 1 -1  1]]\n",
      "1.0\n",
      "{'N': 20, 'W': 0, 'D': 0, 'L': 20, 'Q': -1.0, 'P': 0}\n"
     ]
    }
   ],
   "source": [
    "key = keys[2]\n",
    "state = edge_statistics[key]\n",
    "initial_state, final_state = key.split(\"2\")\n",
    "initial_arr = board.str2arr(initial_state)\n",
    "final_arr = board.str2arr(final_state)\n",
    "\n",
    "print((final_arr - initial_arr).sum())\n",
    "print(final_arr)\n",
    "p_type = (final_arr - initial_arr).sum()\n",
    "Q_final = state[\"Q\"]*p_type\n",
    "print(Q_final)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  0,  0],\n",
       "       [-1,  0,  1],\n",
       "       [ 1, -1,  1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.board.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yidinghou/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9764367]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.stack([[final_arr]],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,y in enumerate(Y) if y ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59890914]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.5 , 0.23, 0.  , 0.02, 0.17, 0.  , 0.01, 0.07]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[1, 0, 0],\n",
    "                 [0, -1,  0],\n",
    "                 [1,  -1,  1]])\n",
    "\n",
    "pred, prob = model.predict(np.stack([[test]],-1))\n",
    "print(pred)\n",
    "prob.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08, 0.52, 0.  , 0.12, 0.  , 0.25, 0.01, 0.  , 0.01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[0, 0, 0],\n",
    "                 [0, -1,  0],\n",
    "                 [1,  -1,  1]])\n",
    "\n",
    "model.predict(np.stack([[test]],-1)).round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import player\n",
    "import numpy as np\n",
    "import keras\n",
    "import neural_network\n",
    "import game\n",
    "\n",
    "model = keras.models.load_model('./best_keras_model.tf')\n",
    "\n",
    "N_games = 200\n",
    "global_step = 50000\n",
    "nn_check_pt = neural_network.nn_predictor.CHECK_POINTS_NAME + '-' + str(global_step)\n",
    "\n",
    "player1 = player.Zero_Player('x', 'Bot_ZERO', nn_type=nn_check_pt, temperature=0)\n",
    "player2 = player.Zero_Player('o', 'Bot_ZERO', nn_type=nn_check_pt, temperature=0)\n",
    "\n",
    "player1.keras_nn = model\n",
    "player2.keras_nn = model\n",
    "\n",
    "# player1.value_estimate =\"nn\"\n",
    "# player2.value_estimate =\"nn\"\n",
    "\n",
    "z_vs_r_game = game.Game(player1, player2)\n",
    "w1, w2 = z_vs_r_game.play_symmetric(N_games)\n",
    "print('{} vs {} summary:'.format(player1.name, player2.name))\n",
    "print('wins={}, draws={}, losses={}'.format(w1, N_games-w1-w2, w2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[0, 0, 0],\n",
    "                 [0, -1,  0],\n",
    "                 [1,  -1,  1]])\n",
    "\n",
    "player2.keras_nn.predict(np.stack([[test]],-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1.keras_nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
